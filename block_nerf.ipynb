{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(array: np.ndarray):\n",
    "    print(np.array2string(array, separator=\", \", formatter={\"float_kind\": lambda x: \"%.4f\" % x}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transforms(path: Path, splits: int):\n",
    "    transforms = load_json(path)\n",
    "    frames = transforms[\"frames\"]\n",
    "    split_frames = np.array_split(frames, splits)\n",
    "    \n",
    "    split_indexes = []\n",
    "    split_transforms = []\n",
    "    for split in split_frames:\n",
    "        split_transforms.append(\n",
    "            {\n",
    "                \"camera_model\": transforms[\"camera_model\"],\n",
    "                \"fl_x\": transforms[\"fl_x\"],\n",
    "                \"fl_y\": transforms[\"fl_y\"],\n",
    "                \"cx\": transforms[\"cx\"],\n",
    "                \"cy\": transforms[\"cy\"],\n",
    "                \"w\": transforms[\"w\"],\n",
    "                \"h\": transforms[\"h\"],\n",
    "                \"k1\": transforms[\"k1\"],\n",
    "                \"k2\": transforms[\"k2\"],\n",
    "                \"p1\": transforms[\"p1\"],\n",
    "                \"p2\": transforms[\"p2\"],\n",
    "                \"frames\": split.tolist(),\n",
    "            }\n",
    "        )\n",
    "        if (len(split_indexes) == 0):\n",
    "            split_indexes.append(len(split))\n",
    "        else:\n",
    "            split_indexes.append(split_indexes[-1] + len(split))\n",
    "    return split_transforms, split_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_path = Path.cwd() / \"block_nerf\" / \"baseline_transforms.json\"\n",
    "split_transforms, split_indexes = split_transforms(transforms_path, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[275, 550, 824, 1098]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the split transforms by themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the original, non-scaled and non-centered transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dataparser_transforms to scale and rotate the transforms\n",
    "dataparser_transforms_path = Path.cwd() / \"block_nerf\" / \"dataparser_transforms.json\"\n",
    "dataparser_transforms = load_json(dataparser_transforms_path)\n",
    "\n",
    "dp_t = np.array(dataparser_transforms[\"transform\"])\n",
    "dp_scale = dataparser_transforms[\"scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0000, -1.0000, -0.0000, -3.4461],\n",
      " [0.9848, -0.0000, 0.1736, 106.3866],\n",
      " [-0.1736, -0.0000, 0.9848, -2.3626],\n",
      " [0.0000, 0.0000, 0.0000, 1.0000]]\n"
     ]
    }
   ],
   "source": [
    "# Use the dp_t and dp_scale to scale and rotate the camera_to_world matrices in the camera_path file such that they're comparable to the split_transforms\n",
    "\n",
    "transforms_matrix = np.array(split_transforms[0][\"frames\"][0][\"transform_matrix\"])\n",
    "pretty_print(transforms_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0387, 0.0485, 0.9981, 0.4484],\n",
      " [0.9993, -0.0019, -0.0387, 0.9853],\n",
      " [-0.0000, 0.9988, -0.0485, -0.0018],\n",
      " [0.0000, 0.0000, 0.0000, 1.0000]]\n"
     ]
    }
   ],
   "source": [
    "camera_path_path = Path.cwd() / \"block_nerf\" / \"camera_path_one_lap.json\"\n",
    "camera_path = load_json(camera_path_path)\n",
    "camera_path_c2w = np.array(camera_path[\"camera_path\"][0][\"camera_to_world\"]).reshape(4,4,)\n",
    "pretty_print(camera_path_c2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0002, 0.9988, -0.0485, 0.2364],\n",
      " [0.9992, -0.0017, -0.0388, -0.3360],\n",
      " [-0.0389, -0.0485, -0.9981, -0.0437],\n",
      " [0.0000, 0.0000, 0.0000, 1.0000]]\n"
     ]
    }
   ],
   "source": [
    "a = dp_t @ camera_path_c2w\n",
    "a[:3, 3] *= dp_scale\n",
    "a = np.vstack((a, [0, 0, 0, 1]))\n",
    "\n",
    "pretty_print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.81123024696991"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the distance between the two matrices trasform_matrix[:3, 3] and camera_to_world[:3, 3]\n",
    "np.linalg.norm(transforms_matrix[:3, 3] - a[:3, 3])\n",
    "# transforms_matrix[:3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0387, 0.9993, -0.0000, 0.0000],\n",
      " [0.0485, -0.0019, 0.9988, 0.0000],\n",
      " [0.9981, -0.0387, -0.0485, 0.0000],\n",
      " [0.4484, 0.9853, -0.0018, 1.0000]]\n"
     ]
    }
   ],
   "source": [
    "b = [0.03871940596928383,0.9992501550311972,-2.220446121113367e-16,0,0.04846804299786681,-0.0018780620887794875,0.9988229486229716,0,0.9980739592677991,-0.03867383019325542,-0.04850441561635006,0,0.4483894695235361,0.985300560475246,-0.0018124304538501418,1]\n",
    "b = np.array(b).reshape(4,4,)\n",
    "pretty_print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.eval_utils import eval_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[13:36:38] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                                 <a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#337\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[13:36:38]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m1\u001b[0m                                                 \u001b]8;id=324917;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=961150;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#337\u001b\\\u001b[2m337\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split train.                                         <a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                         \u001b]8;id=993428;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=109979;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split test.                                          <a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split test.                                          \u001b]8;id=829194;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=61821;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "outputs/exp_combined_baseline_2/nerfacto/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-10_140345/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000034999.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "outputs/exp_combined_baseline_2/nerfacto/\u001b[1;36m2023\u001b[0m-\u001b[1;36m04\u001b[0m-10_140345/nerfstudio_models/step-\u001b[1;36m000034999.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Config-path\n",
    "load_config = Path(\"data/images/exp_combined_baseline_2/exp_combined_baseline_2/nerfacto/2023-04-10_140345/config.yml\")\n",
    "\n",
    "eval_num_rays_per_chunk = 1 << 15 # Same as 2^15\n",
    "\n",
    "_, pipeline, _ = eval_setup(\n",
    "    load_config,\n",
    "    eval_num_rays_per_chunk=eval_num_rays_per_chunk,\n",
    "    test_mode=\"inference\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(camera_path[\"camera_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.0797e-06, -1.6169e-04,  1.0000e+00,  1.8667e+01],\n",
       "        [-1.6169e-04,  1.0000e+00,  1.6169e-04, -2.7510e+01],\n",
       "        [-1.0000e+00, -1.6169e-04, -6.0797e-06, -2.9981e+00]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = pipeline.datamanager.train_dataparser_outputs\n",
    "outputs.dataparser_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7381e-01,  6.0797e-06,  9.8478e-01,  2.0630e-01],\n",
       "         [ 9.8478e-01,  1.6165e-04,  1.7381e-01,  9.9909e-01],\n",
       "         [-1.5813e-04,  1.0000e+00, -3.4057e-05,  5.4568e-03]],\n",
       "\n",
       "        [[ 1.7349e-01,  6.0797e-06,  9.8484e-01,  2.0630e-01],\n",
       "         [ 9.8484e-01,  1.6165e-04, -1.7349e-01,  9.9909e-01],\n",
       "         [-1.6024e-04,  1.0000e+00,  2.2082e-05,  5.4568e-03]],\n",
       "\n",
       "        [[-1.7381e-01,  6.0797e-06,  9.8478e-01,  2.0630e-01],\n",
       "         [ 9.8478e-01,  1.6165e-04,  1.7381e-01,  9.9909e-01],\n",
       "         [-1.5813e-04,  1.0000e+00, -3.4057e-05,  4.2312e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4479e-01,  5.6213e-05,  9.8946e-01,  2.2548e-01],\n",
       "         [ 9.8946e-01,  1.4931e-04,  1.4479e-01,  9.9805e-01],\n",
       "         [-1.3960e-04,  1.0000e+00, -7.7214e-05, -1.7221e-04]],\n",
       "\n",
       "        [[ 2.0235e-01,  5.9315e-05,  9.7931e-01,  2.2548e-01],\n",
       "         [ 9.7931e-01,  1.4945e-04, -2.0235e-01,  9.9805e-01],\n",
       "         [-1.5836e-04,  1.0000e+00, -2.7819e-05, -1.7221e-04]],\n",
       "\n",
       "        [[ 2.0107e-01,  3.7726e-05,  9.7958e-01,  2.1536e-01],\n",
       "         [ 9.7958e-01,  1.5288e-04, -2.0107e-01,  9.9831e-01],\n",
       "         [-1.5734e-04,  1.0000e+00, -6.1886e-06, -1.7229e-04]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.cameras.camera_to_worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000, 1.0000, 0.0001, -3.0017],\n",
      " [-0.1361, 0.0001, -0.9907, 61.9833],\n",
      " [-0.9907, 0.0000, 0.1361, -64.2367],\n",
      " [0.0000, 0.0000, 0.0000, 1.0000]]\n"
     ]
    }
   ],
   "source": [
    "a = pipeline.datamanager.train_dataparser_outputs.transform_poses_to_original_space(outputs.cameras.camera_to_worlds).numpy()\n",
    "b = a[200]\n",
    "original = np.vstack((b, [0, 0, 0, 1]))\n",
    "pretty_print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.15940801618345"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(transforms_matrix[:3, 3] - original[:3, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with exp_combined_baseline_block_nerf_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = Path.cwd() / \"data\" / \"images\" / \"exp_combined_baseline_block_nerf_2\"\n",
    "transforms_path = exp_path / \"0\" / \"transforms.json\"\n",
    "transforms = load_json(transforms_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[13:09:26] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                                 <a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#337\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[13:09:26]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m1\u001b[0m                                                 \u001b]8;id=582482;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=12523;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#337\u001b\\\u001b[2m337\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split train.                                         <a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                         \u001b]8;id=941539;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=937736;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split test.                                          <a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split test.                                          \u001b]8;id=865993;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=911845;file:///lhome/olesto/code/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#165\u001b\\\u001b[2m165\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "data/images/exp_combined_baseline_block_nerf_2/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>/exp_combined_baseline_block_nerf_2-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>/nerfacto/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-11_130124/nerfstu\n",
       "dio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000001999.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "data/images/exp_combined_baseline_block_nerf_2/\u001b[1;36m0\u001b[0m/exp_combined_baseline_block_nerf_2-\u001b[1;36m0\u001b[0m/nerfacto/\u001b[1;36m2023\u001b[0m-\u001b[1;36m04\u001b[0m-11_130124/nerfstu\n",
       "dio_models/step-\u001b[1;36m000001999.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the experiment's pipeline\n",
    "load_config = exp_path / \"0/exp_combined_baseline_block_nerf_2-0/nerfacto/2023-04-11_130124/config.yml\"\n",
    "\n",
    "eval_num_rays_per_chunk = 1 << 15 # Same as 2^15\n",
    "\n",
    "_, pipeline, _ = eval_setup(\n",
    "    load_config,\n",
    "    eval_num_rays_per_chunk=eval_num_rays_per_chunk,\n",
    "    test_mode=\"inference\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.87194047e-02,  4.84680439e-02,  9.98073973e-01,\n",
       "         4.48389470e-01],\n",
       "       [ 9.99250123e-01, -1.87806213e-03, -3.86738307e-02,\n",
       "         9.85300560e-01],\n",
       "       [-2.01661604e-16,  9.98822968e-01, -4.85044163e-02,\n",
       "        -1.81243045e-03],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline.datamanager.train_dataparser_outputs.transform_poses_to_original_space(outputs.cameras.camera_to_worlds).numpy()\n",
    "\n",
    "# Transform the camera_path to the original space\n",
    "camera_path_c2w = np.array(camera_path[\"camera_path\"][0][\"camera_to_world\"]).reshape(4,4,)\n",
    "# camera_path_c2w = torch.tensor(camera_path_c2w)[:, :-1, :]\n",
    "# camera_path_c2w = camera_path_c2w.float()\n",
    "\n",
    "camera_path_c2w\n",
    "# a = pipeline.datamanager.train_dataparser_outputs.transform_poses_to_original_space(camera_path_c2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0000, 0.0127, -0.0006, 0.2364],\n",
      " [0.0127, -0.0000, -0.0005, -0.3360],\n",
      " [-0.0005, -0.0006, -0.0126, -0.0437]]\n"
     ]
    }
   ],
   "source": [
    "t = pipeline.datamanager.train_dataparser_outputs.dataparser_transform\n",
    "s = pipeline.datamanager.train_dataparser_outputs.dataparser_scale\n",
    "# camera_path_c2w\n",
    "a = (t @ camera_path_c2w) * s\n",
    "a = a.numpy()\n",
    "pretty_print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty_print(outputs.cameras.camera_to_worlds[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0022, 0.0000, 0.0125, 0.2063],\n",
      " [0.0125, 0.0000, 0.0022, 0.9991],\n",
      " [-0.0000, 0.0127, -0.0000, 0.0055]]\n"
     ]
    }
   ],
   "source": [
    "original_transforms = np.array(transforms[\"frames\"][0][\"transform_matrix\"])\n",
    "# pretty_print(original_transforms)\n",
    "\n",
    "transformed_transforms = ((t @ original_transforms) * s).numpy()\n",
    "pretty_print(transformed_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a85e529c05ee86e5cc8c5dd84ffabb9108ebd802a12a42146762b6f4dab3099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
